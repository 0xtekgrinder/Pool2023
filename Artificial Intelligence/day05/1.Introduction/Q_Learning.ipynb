{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from seaborn import heatmap\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Import the Environment class from the envi module\n",
    "from envi import Environment\n",
    "\n",
    "# Define the actions that the agent can take\n",
    "ACTIONS = {'UP': 0, 'LEFT': 1, 'DOWN': 2, 'RIGHT': 3}\n",
    "\n",
    "# Define the size of the gridworld\n",
    "MAP_SIZE = 50\n",
    "\n",
    "# Define the number of episodes to train for\n",
    "EPISODES = 10_000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    This class defines our Agent which will interact with the environment and update its Q Table\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize the Q Table for the agent with zeros\n",
    "        self.q_table = np.zeros((MAP_SIZE ** 2, len(ACTIONS)))\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        \"\"\"\n",
    "        This method picks the most valuable action for the given state from the Q Table\n",
    "        \"\"\"\n",
    "        # Return the action that has the highest value in the Q Table for the given state\n",
    "        return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q_table(self, new_state, state, action, reward):\n",
    "        \"\"\"\n",
    "        This method updates the Q Table\n",
    "        \"\"\"\n",
    "        # Estimate the optimal future value of the new state\n",
    "        estimate_of_optimal_future_value = max(self.q_table[new_state])\n",
    "        \n",
    "        # Calculate the new value for the given state and action\n",
    "        new_value = reward + 0.99 * estimate_of_optimal_future_value\n",
    "        \n",
    "        # Calculate the temporal difference between the old and new values\n",
    "        temporal_difference = new_value - self.q_table[state,action]\n",
    "        \n",
    "        # Update the Q Table for the given state and action\n",
    "        self.q_table[state,action] += 0.05 * temporal_difference\n",
    "        \n",
    "        # Return the absolute value of the temporal difference\n",
    "        return abs(temporal_difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the Agent class, which represents an agent that interacts with an environment and updates its Q-table based on its experiences. The Agent class has three methods:\n",
    "\n",
    "**`__init__`**: This method initializes the Agent and sets the q_table to a 2D numpy array of zeros with dimensions MAP_SIZE ** 2 by len(ACTIONS).\\\n",
    "**`greedy_action`**: This method picks the action with the highest value in the Q-table for a given state.\\\n",
    "**`update_q_table`**: This method updates the Q-table for a given state and action, based on the reward obtained and the estimated optimal future value of the new state. It also returns the absolute value of the temporal difference between the old and new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment and an agent\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "env = Environment(MAP_SIZE, ACTIONS)\n",
    "agent = Agent()\n",
    "\n",
    "# Initialize empty lists for rewards and losses\n",
    "recent_rewards = deque(maxlen=1_000)\n",
    "train_rewards = []\n",
    "train_loss = []\n",
    "\n",
    "# Initialize the exploration rate\n",
    "epsilon = 1\n",
    "\n",
    "# Iterate over the number of episodes\n",
    "for episode in range(EPISODES):\n",
    "    # Reset the environment to get the initial state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Initialize empty lists for rewards and losses in this episode\n",
    "    episode_reward = []\n",
    "    episode_loss = []\n",
    "\n",
    "    # Iterate over the time steps in the episode\n",
    "    for i in range(1000):\n",
    "        # Decrease the exploration rate over time\n",
    "        epsilon = max(epsilon * 0.995, 0.05)\n",
    "        # Choose an action\n",
    "        greed = random.random()\n",
    "        if greed > epsilon:\n",
    "            # Choose the greedy action\n",
    "            action = agent.greedy_action(state)\n",
    "        else:\n",
    "            # Choose a random action\n",
    "            action = random.randint(0, len(ACTIONS) - 1)\n",
    "\n",
    "        # Interact with the environment to get the new state, reward, and done flag\n",
    "        new_state, reward, done = env.step(action)\n",
    "        episode_reward.append(reward)\n",
    "\n",
    "        # Update the Q-table and get the loss\n",
    "        loss = agent.update_q_table(new_state, state, action, reward)        \n",
    "        episode_loss.append(loss)\n",
    "\n",
    "        # Set the new state as the current state\n",
    "        state = new_state\n",
    "\n",
    "        # If the episode is done, break out of the loop\n",
    "        if done is True:\n",
    "            break\n",
    "    \n",
    "    # Log the rewards and losses for this episode\n",
    "    train_rewards.append(np.sum(episode_reward))\n",
    "    recent_rewards.append(train_rewards[-1])\n",
    "    train_loss.append(np.mean(episode_loss))\n",
    "\n",
    "    # Print a table of information about the episode every 5,000 episodes\n",
    "    if episode % 1_000 == 0:\n",
    "        print(f\"Episode {episode:>6}: \\tR:{np.mean(recent_rewards):>6.3f}\\t Epsilon:{epsilon:>6.3f}\\t State:{state:>6}\")\n",
    "\n",
    "# Reset the environment to get the initial state\n",
    "state = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an Environment object and an Agent object. It then runs a number of episodes, in which the agent interacts with the environment, updates its Q-table, and logs the rewards and losses. The exploration rate is decreased over time to encourage the agent to exploit its knowledge of the environment rather than explore it. After the episodes are run, the environment is reset to its initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# plotting rewards\n",
    "ax[0].plot(gaussian_filter1d(train_rewards, sigma=10))\n",
    "ax[0].set_title('Rewards')\n",
    "# plotting loss\n",
    "ax[1].plot(gaussian_filter1d(train_loss, sigma=10), color='red')\n",
    "ax[1].set_title('Loss')\n",
    "# show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the optimal actions from the Q-table\n",
    "best_actions = [np.argmax(x) if np.mean(x) != x[0] else -1 for x in agent.q_table]\n",
    "\n",
    "# Initialize a matrix for the policy\n",
    "policy = np.zeros((MAP_SIZE ** 2, len(ACTIONS)))\n",
    "\n",
    "# Fill in the policy matrix\n",
    "for y in range(MAP_SIZE ** 2):\n",
    "    for x in range(MAP_SIZE):\n",
    "        if x == best_actions[y]:\n",
    "            policy[y][x] = 1\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# Plot the policy matrix as a heatmap\n",
    "heatmap(policy, ax=ax[0], xticklabels=ACTIONS, cbar=False)\n",
    "\n",
    "# Plot the Q-table as a heatmap\n",
    "heatmap(agent.q_table, ax=ax[1], xticklabels=ACTIONS, annot=MAP_SIZE<6)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code extracts the optimal actions from the Q-table and uses them to create a matrix representing the policy. It then plots the policy matrix and the Q-table as heatmaps. The policy matrix shows which actions are optimal in which states, while the Q-table shows the values of the actions in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell matplotlib to show the plots inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the path to the FFmpeg binary\n",
    "plt.rcParams['animation.ffmpeg_path'] = 'ffmpeg'\n",
    "\n",
    "# Initialize an empty list of frames\n",
    "frames = []\n",
    "\n",
    "# Create a figure with a single subplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Iterate over the time steps in the episode\n",
    "for i in range(1000):\n",
    "    # Add the current state of the environment to the list of frames\n",
    "    frames.append([ax.imshow(env.graphic(), animated=True)])\n",
    "\n",
    "    # Choose the greedy action for the current state\n",
    "    action = agent.greedy_action(state)\n",
    "\n",
    "    # Interact with the environment to get the new state, reward, and done flag\n",
    "    new_state, reward, done = env.step(action)\n",
    "\n",
    "    # Set the new state as the current state\n",
    "    state = new_state\n",
    "\n",
    "    # If the episode is done, reset the environment and break out of the loop\n",
    "    if done is True:\n",
    "        frames.append([ax.imshow(env.graphic(), animated=True)])\n",
    "        state = env.reset()\n",
    "        break\n",
    "\n",
    "# Create an animation from the list of frames\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True)\n",
    "\n",
    "# Convert the animation to an HTML5 video\n",
    "video = ani.to_html5_video()\n",
    "\n",
    "# Display the video in the notebook\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an animation that shows the agent interacting with the environment and following the optimal policy learned from the Q-table. It iterates over the time steps in the episode, chooses the greedy action for the current state, and updates the environment. When the episode is done, it resets the environment and breaks out of the loop. The frames are then used to create an animation, which is converted to an HTML5 video and displayed in the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pool')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b483bbea0ef867292651300ca303e9b91f9a0c7db919f54df8d16a1790f2d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

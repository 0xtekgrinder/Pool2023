{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Set the learning rate and discount factor\n",
    "lr = 5e-3\n",
    "gamma = 0.995\n",
    "\n",
    "# Set the number of episodes to run\n",
    "episodes = 1000\n",
    "\n",
    "# Set the environment to use\n",
    "env_name = 'CartPole-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network to model the policy\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create fully-connected layers with ReLU activations\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 64)\n",
    "        self.fc2 = nn.Linear(64, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the input tensor to a float tensor\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # Apply ReLU activations to the fully-connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Apply a softmax activation to the final layer, to get probabilities for each action\n",
    "        x = F.softmax(self.fc2(x), dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create the environment and the neural network\n",
    "env = gym.make(env_name)\n",
    "network = NeuralNetwork(env);\n",
    "\n",
    "# Use Adam optimizer to optimize the neural network\n",
    "optim = torch.optim.Adam(network.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we define a class NeuralNetwork that represents the policy model for the agent. The NeuralNetwork class inherits from nn.Module, which is the base class for all neural network models in PyTorch.\n",
    "\n",
    "The `__init__` method of the NeuralNetwork class takes an env argument, which represents the environment that the agent is interacting with. This method initializes the base class using `super().__init__()`, and then creates three fully-connected layers with ReLU activations. The first and second layers have 128 and 64 units, respectively, while the third layer has a number of units equal to the number of actions in the environment's action space.\n",
    "\n",
    "The `forward` method of the NeuralNetwork class takes an input tensor x, and applies the fully-connected layers with ReLU activations to it. The final layer applies a softmax activation, which outputs probabilities for each action in the environment's action space.\n",
    "\n",
    "After defining the NeuralNetwork class, we create an instance of the environment using `gym.make(env_name)`. We then create an instance of the NeuralNetwork class, passing the environment as an argument.\n",
    "\n",
    "Finally, we use the Adam optimizer from the torch.optim module to optimize the parameters of the NeuralNetwork instance. This optimizer will be used to update the probabilities in the distribution, based on the rewards the agent receives while interacting with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the deque class from the collections module\n",
    "from collections import deque\n",
    "\n",
    "# Initialize empty lists for rewards and losses\n",
    "train_rewards = []\n",
    "train_loss = []\n",
    "recent_rewards = deque(maxlen=100)\n",
    "\n",
    "# Iterate over the number of episodes\n",
    "for episode in range(episodes):\n",
    "    # Reset the environment and initialize empty lists for actions, states, and rewards\n",
    "    state, _  = env.reset()\n",
    "    Actions, States, Rewards = [], [], []\n",
    "\n",
    "    # Train the agent for a single episode\n",
    "    for _ in range(1000):\n",
    "        # Get the probabilities for each action, using the current state\n",
    "        probs = network.forward(state)\n",
    "\n",
    "        # Sample an action from the distribution\n",
    "        action = Categorical(probs).sample().item()\n",
    "\n",
    "        # Take the action in the environment and get the new state, reward, and done flag\n",
    "        new_state, rew, done, trunc, _ = env.step(action)\n",
    "\n",
    "        # Save the action, state, and reward for later\n",
    "        Actions.append(torch.tensor(action, dtype=torch.int))\n",
    "        States.append(state)\n",
    "        Rewards.append(rew)\n",
    "\n",
    "        # Update the current state with the new state\n",
    "        state = new_state\n",
    "\n",
    "        # If the episode is done or the time limit is reached, stop training\n",
    "        if done or trunc:\n",
    "            break\n",
    "        \n",
    "    ## Discount the returns using the discount factor\n",
    "    DiscountedReturns = []\n",
    "    for t in range(len(Rewards)):\n",
    "        G = 0.0\n",
    "        for k, r in enumerate(Rewards[t:]):\n",
    "            G += gamma**k * r\n",
    "        DiscountedReturns.append(G)\n",
    "    \n",
    "    # Perform gradient ascent to update the probabilities in the distribution\n",
    "    for State, Action, G in zip(States, Actions, DiscountedReturns):\n",
    "        # Get the probabilities for the current state\n",
    "        probs = network.forward(State)\n",
    "\n",
    "        # Calculate the loss as the negative log probability of the chosen action\n",
    "        # multiplied by the discounted return\n",
    "        loss = - Categorical(probs).log_prob(Action) * G\n",
    "\n",
    "        # Clear the gradients, backpropagate the loss, and update the network parameters\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # Save the total reward for the episode and append it to the recent rewards queue\n",
    "    train_rewards.append(np.sum(Rewards))\n",
    "    recent_rewards.append(train_rewards[-1])\n",
    "\n",
    "    # Print the mean recent reward every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode:>6}: \\tR:{np.mean(recent_rewards):>6.3f}\")\n",
    "\n",
    "    # If the mean recent reward is greater than 200, stop training\n",
    "    if np.mean(recent_rewards) > 200:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_rewards)\n",
    "ax.plot(gaussian_filter1d(train_rewards, sigma=20), linewidth=4)\n",
    "ax.set_title('Rewards')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment using the gym.make() method and setting the \n",
    "# render_mode to \"human\" to enable human-readable output.\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "# Run the trained agent for five episodes, with each episode lasting for a\n",
    "# maximum of 1000 steps.\n",
    "for _ in range(5):\n",
    "    # Initialize empty list for rewards for the current episode\n",
    "    Rewards = []\n",
    "    \n",
    "    # Reset the environment and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Render the initial state\n",
    "    env.render()\n",
    "    \n",
    "    # Run the agent for 1000 steps or until the episode ends\n",
    "    for _ in range(1000):\n",
    "        # Calculate the probabilities of taking each action using the trained\n",
    "        # neural network\n",
    "        probs = network.forward(state)\n",
    "        \n",
    "        # Sample an action from the resulting distribution using the \n",
    "        # torch.distributions.Categorical() method\n",
    "        c = torch.distributions.Categorical(probs=probs)        \n",
    "        action = c.sample().item()\n",
    "        \n",
    "        # Take the action and get the new state, reward, and done flag from the\n",
    "        # environment\n",
    "        new_state, reward, done, trunc, _ = env.step(action)\n",
    "        \n",
    "        # Render the new state\n",
    "        env.render()\n",
    "\n",
    "        # Update the current state for the next iteration\n",
    "        state = new_state\n",
    "\n",
    "        # Add the reward to the total rewards for the current episode\n",
    "        Rewards.append(reward)\n",
    "\n",
    "        # If the episode has ended (because the maximum number of steps is reached\n",
    "        # or because the done flag is set), break out of the loop\n",
    "        if done or trunc:\n",
    "            break\n",
    "    \n",
    "    # Print the total rewards for the current episode\n",
    "    print(f'Reward: {sum(Rewards)}')\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
